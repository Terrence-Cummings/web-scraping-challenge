{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Homework - Mission to Mars\n",
    "## Terrence Cummings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping\n",
    "\n",
    "The outputs of 5 scraping exercises are:\n",
    "\n",
    "1. Dataframe containing the date, title, and summary of the latest news articles about mars from the NASA Mars News site.\n",
    "\n",
    "2. A URL for the full-sized featured image at the NASA Jet Propulsion Laboratory website.\n",
    "\n",
    "3. Text of the most recent weather posting from the Mars Weather Twitter featured\n",
    "\n",
    "4. An HTML table of key facts about Mars\n",
    "\n",
    "5. A list of dictionaries with the name and a link to a full-size image of each of Mars' four hemispheres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars New\n",
    "\n",
    "Retrieve the following data from a search of the latest news articles at the NASA Mars News website URL:\n",
    "\n",
    "1. Publication Date\n",
    "\n",
    "2. Title\n",
    "\n",
    "3. Summary paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Because the search results at the URL are from Javascript use Selenium to scrape the data\n",
    "\n",
    "#URL for NASA Mars News website. This show 40 articles from a search of the criteria \"Latest\" and \"All Categories\".\n",
    "#Results of the search are generated by Javascript so not viewable in the webpage HTML\n",
    "url_mars_news = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "\n",
    "#Initialize lists to store Selenium objects\n",
    "dates = []\n",
    "titles = []\n",
    "summarys = []\n",
    "\n",
    "#Use Selenium to get the needed fields from the JS results\n",
    "#XPath for tags were found by right-clicking on the tag in the Chrome Inspector tool the Copy XPath\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url_mars_news)\n",
    "\n",
    "#Add a delay to give the scraper time to acquire the data\n",
    "time.sleep(10)\n",
    "dates = driver.find_elements_by_xpath('//*[@id=\"page\"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[1]')\n",
    "titles = driver.find_elements_by_xpath('//*[@id=\"page\"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[2]/a')\n",
    "summarys = driver.find_elements_by_xpath('//*[@id=\"page\"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[3]')\n",
    "\n",
    "# create empty array to store text data extracted from Selenium objects\n",
    "date_lst = []\n",
    "title_lst = []\n",
    "summary_lst = []\n",
    "\n",
    "# loop over results and extract text from Selenium objects, add to each list\n",
    "for date in dates:\n",
    "    article_date = date.text\n",
    "    date_lst.append(article_date)\n",
    "for title in titles:\n",
    "    article_title = title.text\n",
    "    title_lst.append(article_title)\n",
    "for summary in summarys:\n",
    "    article_summary = summary.text\n",
    "    summary_lst.append(article_summary)\n",
    "\n",
    "#Make dataframe of NASA Mars Latest News Articles\n",
    "nasa_mars_articles_df = pd.DataFrame(list(zip(date_lst, title_lst, summary_lst)), columns =['Date', 'Title', 'Summary'])\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to dictionary and confirm results of the scraping\n",
    "nasa_mars_articles_dict = nasa_mars_articles_df.to_dict('records')\n",
    "nasa_mars_articles_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Images\n",
    "Get the URL for the featured image at the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Setup Splinter Browsder and target URL\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url_jpl = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "#Go to URL and navigate to page with full size image.\n",
    "browser.visit(url_jpl)\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "#Grab the HTM from the webpage with the full size image which contains the link to that image\n",
    "html = browser.html\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#Find the image tag for the main image\n",
    "main_img = soup.find('img', class_='main_image')\n",
    "\n",
    "#Extract the source link for the image\n",
    "main_img_url = main_img['src']\n",
    "\n",
    "#Build the full URL to the full size featured image\n",
    "main_img_url_full = 'https://www.jpl.nasa.gov'+main_img_url\n",
    "\n",
    "#Check result\n",
    "main_img_url_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather\n",
    "Get the latest Mars Weather tweet from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Selenium because Twitter tweets are populated by JS\n",
    "url_mars_tweet = 'https://twitter.com/marswxreport?lang=en'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url_mars_tweet)\n",
    "time.sleep(1)\n",
    "\n",
    "#Get the first Mars weather tweet using Xpath in Selenium\n",
    "mars_weather_tweet_obj = driver.find_elements_by_xpath('//*[@id=\"react-root\"]/div/div/div[2]/main/div/div/div/div[1]/div/div/div/div/div[2]/section/div/div/div/div[1]/div/div/div/div/article/div/div[2]/div[2]/div[2]/div[1]/div/span')\n",
    "\n",
    "#Extract the text of the tweet and replace line breaks\n",
    "mars_weather_tweet = mars_weather_tweet_obj[0].text.replace('\\n',', ')\n",
    "\n",
    "#Close browser\n",
    "driver.quit()\n",
    "\n",
    "#Check result\n",
    "mars_weather_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts\n",
    "Get table of Mars facts using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send Pandas to read tables from URL\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "mars_facts = pd.read_html(mars_facts_url)\n",
    "\n",
    "#Grab the first table of facts, add column headings\n",
    "mars_facts_df = mars_facts[0]\n",
    "mars_facts_df.columns = ['Parameter', 'Fact']\n",
    "\n",
    "#Write as HTML table\n",
    "mars_facts_df.to_html('mars_facts_table.html', index=False)\n",
    "\n",
    "#Convert df to dictionary\n",
    "mars_facts_dict = dict(zip(mars_facts_df.Parameter, mars_facts_df.Fact))\n",
    "\n",
    "#Check results\n",
    "mars_facts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres\n",
    "\n",
    "Create a list of dictionaries containing the URL's and Titles for images of Mars' hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Splinter Browsder and target URL\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "mars_hemis_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "#Go to URL that summarizes the Mars hemispheres.\n",
    "browser.visit(mars_hemis_url)\n",
    "\n",
    "#Grab the HTML\n",
    "html2 = browser.html\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use BeautifulSoup to parse the HTML\n",
    "soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "#Find the URL tag for each hemisphere's separate page\n",
    "hemi_links = soup2.find_all('a', class_='itemLink')\n",
    "\n",
    "#Build a list of the full URL for each hemisphere's separate page so we can go there to find the link to download the full size image.\n",
    "full_urls = []\n",
    "for link in hemi_links:\n",
    "    full_url = 'https://astrogeology.usgs.gov/'+link['href']\n",
    "    full_urls.append(full_url)\n",
    "\n",
    "#Remove duplicates from the URL list\n",
    "full_urls = list(dict.fromkeys(full_urls))\n",
    "\n",
    "#Check that we have a working URL for each hempisphere page\n",
    "full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Splinter browser\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Initialize the list of dictionaries that will hold each hemisphere's title and link to full size image download\n",
    "mars_hems_dict_lst = []\n",
    "\n",
    "#For each hemispher URL\n",
    "for i in full_urls:\n",
    "    #Go to the individual webpage of that hemisphere\n",
    "    browser.visit(i)\n",
    "    #Grab the HTML\n",
    "    html3 = browser.html\n",
    "    #Use BeautifulSoup to parse the HTML\n",
    "    soup3 = BeautifulSoup(html3, 'html.parser')\n",
    "    #Find the link for the Original tif photo download (not the sample JPG)\n",
    "    image_link = soup3.find('a', string='Sample')\n",
    "    image_link = image_link['href']\n",
    "    #Find the title or name of the hemisphere\n",
    "    image_title = soup3.find('h2', class_='title')\n",
    "    #Remove unneeded wording at the end of the title\n",
    "    image_title = image_title.text.replace(' Enhanced', '')\n",
    "    #Create a dictionary of the title and link for that hemisphere\n",
    "    temp_dict = {'title': image_title, 'img_url': image_link}\n",
    "    #Add the dictionary to the list\n",
    "    mars_hems_dict_lst.append(temp_dict)\n",
    "\n",
    "browser.quit()\n",
    "#Check the final result\n",
    "mars_hems_dict_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final Mars Data dictionary for Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_data = {\n",
    "    'article': nasa_mars_articles_dict,\n",
    "    'weather': mars_weather_tweet,\n",
    "    'featured_image': main_img_url_full,\n",
    "    'mars_facts': mars_facts_dict,\n",
    "    'mars_hems' : mars_hems_dict_lst\n",
    "}\n",
    "\n",
    "mars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection to mongodb\n",
    "conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Select database and collection to use\n",
    "db = client.mars\n",
    "mars_info = db.mars_info\n",
    "mars_info.insert_one(mars_data)\n",
    "print('Data uploaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = 'mongodb://localhost:27017'\n",
    "# client = pymongo.MongoClient(conn)\n",
    "# db = client.mars\n",
    "# mars_data2 = db.mars_info.find_one(sort=[( '_id', pymongo.DESCENDING )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpythondatacondaab47c43dfe824d4eb95e9e8f15b48370",
   "display_name": "Python 3.6.10 64-bit ('PythonData': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}