import pandas as pd
from bs4 import BeautifulSoup
import json
import requests
import pymongo
from splinter import Browser
from selenium import webdriver
import time
import sys
import os


def scrape_info():
    from selenium.webdriver.chrome.options import Options
    chrome_options = Options()
    chrome_options.add_argument("--headless")   

    #Because the search results at the URL are from Javascript use Selenium to scrape the data

    #URL for NASA Mars News website. This show 40 articles from a search of the criteria "Latest" and "All Categories".
    #Results of the search are generated by Javascript so not viewable in the webpage HTML
    url_mars_news = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'

    #Initialize lists to store Selenium objects
    dates = []
    titles = []
    summarys = []

    #Use Selenium to get the needed fields from the JS results
    #XPath for tags were found by right-clicking on the tag in the Chrome Inspector tool the Copy XPath
    driver = webdriver.Chrome(options=chrome_options)
    driver.get(url_mars_news)

    #Add a delay to give the scraper time to acquire the data
    time.sleep(10)
    dates = driver.find_elements_by_xpath('//*[@id="page"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[1]')
    titles = driver.find_elements_by_xpath('//*[@id="page"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[2]/a')
    summarys = driver.find_elements_by_xpath('//*[@id="page"]/div[3]/div/article/div/section/div/ul/li[*]/div/div/div[3]')

    # create empty array to store text data extracted from Selenium objects
    date_lst = []
    title_lst = []
    summary_lst = []
    news_url_lst = []

    # loop over results and extract text from Selenium objects, add to each list
    for date in dates:
        article_date = date.text
        date_lst.append(article_date)
    for title in titles:
        article_title = title.text
        title_lst.append(article_title)
        href = title.get_attribute('href')
        news_url_lst.append(href)
    for summary in summarys:
        article_summary = summary.text
        summary_lst.append(article_summary)

    #Make dataframe of NASA Mars Latest News Articles
    nasa_mars_articles_df = pd.DataFrame(list(zip(date_lst, title_lst, summary_lst, news_url_lst)), columns =['Date', 'Title', 'Summary', 'URL'])
    driver.quit()

    #Convert to dictionary and confirm results of the scraping
    nasa_mars_articles_dict = nasa_mars_articles_df.to_dict('records')

    #Setup Splinter Browsder and target URL
    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}
    browser = Browser('chrome', **executable_path, headless=True)
    url_jpl = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'

    #Go to URL and navigate to page with full size image.
    browser.visit(url_jpl)
    browser.click_link_by_partial_text('FULL IMAGE')
    browser.click_link_by_partial_text('more info')

    #Grab the HTM from the webpage with the full size image which contains the link to that image
    html = browser.html
    browser.quit()

    #Use BeautifulSoup to parse the HTML
    soup = BeautifulSoup(html, 'html.parser')

    #Find the image tag for the main image
    main_img = soup.find('img', class_='main_image')

    #Extract the source link for the image
    main_img_url = main_img['src']

    #Build the full URL to the full size featured image
    main_img_url_full = 'https://www.jpl.nasa.gov'+main_img_url

    #Use Selenium because Twitter tweets are populated by JS
    url_mars_tweet = 'https://twitter.com/marswxreport?lang=en'
    driver = webdriver.Chrome(options=chrome_options)
    driver.get(url_mars_tweet)
    time.sleep(1)

    #Get the first Mars weather tweet using Xpath in Selenium
    mars_weather_tweet_obj = driver.find_elements_by_xpath('//*[@id="react-root"]/div/div/div[2]/main/div/div/div/div[1]/div/div/div/div/div[2]/section/div/div/div/div[1]/div/div/div/div/article/div/div[2]/div[2]/div[2]/div[1]/div/span')

    #Extract the text of the tweet and replace line breaks
    mars_weather_tweet = mars_weather_tweet_obj[0].text.replace('\n',', ')

    #Close browser
    driver.quit()

    #Send Pandas to read tables from URL
    mars_facts_url = 'https://space-facts.com/mars/'
    mars_facts = pd.read_html(mars_facts_url)

    #Grab the first table of facts, add column headings
    mars_facts_df = mars_facts[0]
    mars_facts_df.columns = ['Parameter', 'Fact']

    #Write as HTML table
    mars_facts_df.to_html('mars_facts_table.html', index=False)

    #Convert df to dictionary
    mars_facts_dict = dict(zip(mars_facts_df.Parameter, mars_facts_df.Fact))

    #Setup Splinter Browsder and target URL
    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}
    browser = Browser('chrome', **executable_path, headless=True)
    mars_hemis_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'

    #Go to URL that summarizes the Mars hemispheres.
    browser.visit(mars_hemis_url)

    #Grab the HTML
    html2 = browser.html
    browser.quit()

    #Use BeautifulSoup to parse the HTML
    soup2 = BeautifulSoup(html2, 'html.parser')

    #Find the URL tag for each hemisphere's separate page
    hemi_links = soup2.find_all('a', class_='itemLink')

    #Build a list of the full URL for each hemisphere's separate page so we can go there to find the link to download the full size image.
    full_urls = []
    for link in hemi_links:
        full_url = 'https://astrogeology.usgs.gov/'+link['href']
        full_urls.append(full_url)

    #Remove duplicates from the URL list
    full_urls = list(dict.fromkeys(full_urls))

    #Setup Splinter browser
    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}
    browser = Browser('chrome', **executable_path, headless=True)

    #Initialize the list of dictionaries that will hold each hemisphere's title and link to full size image download
    mars_hems_dict_lst = []

    #For each hemispher URL
    for i in full_urls:
        #Go to the individual webpage of that hemisphere
        browser.visit(i)
        #Grab the HTML
        html3 = browser.html
        #Use BeautifulSoup to parse the HTML
        soup3 = BeautifulSoup(html3, 'html.parser')
        #Find the link for the Original tif photo download (not the sample JPG)
        image_link = soup3.find('a', string='Sample')
        image_link = image_link['href']
        #Find the title or name of the hemisphere
        image_title = soup3.find('h2', class_='title')
        #Remove unneeded wording at the end of the title
        image_title = image_title.text.replace(' Enhanced', '')
        #Create a dictionary of the title and link for that hemisphere
        temp_dict = {'title': image_title, 'img_url': image_link}
        #Add the dictionary to the list
        mars_hems_dict_lst.append(temp_dict)

    browser.quit()

    mars_data = {
        'article': nasa_mars_articles_dict,
        'weather': mars_weather_tweet,
        'featured_image': main_img_url_full,
        'mars_facts': mars_facts_dict,
        'mars_hems' : mars_hems_dict_lst
    }

    return mars_data
